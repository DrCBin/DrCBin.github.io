---
layout: post
title: "爬虫实战--爬取糗事百科段子"
categories: [tech]
excerpt: 爬虫系列实战!
tags: 
  - 爬虫
---

 首先,手动打开糗事百科的网址,然后正常浏览热门，发现糗事百科热门的网址形成是这样的
 `http://www.qiushibaike.com/8hr/page/3/`:最后面的3代表是第一页

然后使用urllib尝试访问一下,看看能不能下载

```python3
from urllib import request
page = 2
url = 'http://www.qiushibaike.com/8hr/page/' + str(page)

try:
    with request.urlopen(url) as response:
        print(response.read().decode('utf-8'))
except Exception as e:
    print(e)
```

无情的拒绝了,加上用户代理试试

```python
user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'
headers = {'User-Agent':user_agent}
custom_request = request.Request(url, headers = headers)
with request.uelopen(custom_request) as response:
```

再次访问,成功了


打开百科的源码,通过分析可知,每个段子是由一个`class="article bolck untagged md15"`的div包裹着.然后就是写正则表达式来匹配段子了.

```python
date = None

with request.urlopen(custom_request, headers=headers) as response:
    date = response.read().decode('utf-8')
pattern = re.compile('<div.*?author.*?<h2>(?P<auth>.*?)</h2>.*?content.*?<span>(?P<body>.*?)</span>(?P<img>.*?)stats.*?', re.S)

items = re.findall(pattern, date)

for item in items:
    print(item[0], item[1])

```
这样就吧每个段子的作者和内容爬下来了
